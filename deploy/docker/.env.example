# OmniAI Docker Production Environment
# Copy to .env and fill in values. NEVER commit .env!

# =============================================================================
# NGROK TUNNEL (REQUIRED)
# =============================================================================
# Get your auth token from: https://dashboard.ngrok.com/get-started/your-authtoken
NGROK_AUTHTOKEN=

# =============================================================================
# AUTHENTICATION (REQUIRED - generate new values!)
# =============================================================================
# python -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=

# python -c "import secrets; print(secrets.token_hex(16))"
CSRF_SECRET=

# One-time admin bootstrap token
# python -c "import secrets; print(secrets.token_urlsafe(32))"
ADMIN_BOOTSTRAP_TOKEN=

# =============================================================================
# ORIGIN LOCK (REQUIRED for tunnel security)
# =============================================================================
ORIGIN_LOCK_ENABLED=true

# Shared secret - ngrok injects this as X-Origin-Secret header automatically
# python -c "import secrets; print(secrets.token_urlsafe(32))"
ORIGIN_LOCK_SECRET=

# =============================================================================
# COOKIES (REQUIRED for cross-origin GitHub Pages)
# =============================================================================
COOKIE_SECURE=true
COOKIE_SAMESITE=None
SESSION_COOKIE_NAME=omniplexity_session
SESSION_TTL_SECONDS=604800

# =============================================================================
# CORS (REQUIRED - list your frontend origins)
# =============================================================================
# Include your ngrok URL after it's generated (or use wildcard for dev)
CORS_ORIGINS=["https://omniplexity.github.io"]

# =============================================================================
# SERVER (defaults are fine)
# =============================================================================
LOG_LEVEL=INFO
ENVIRONMENT=production
INVITE_ONLY=true

# =============================================================================
# LLM PROVIDERS
# =============================================================================
# For services running on Docker host, use host.docker.internal
LMSTUDIO_BASE_URL=http://host.docker.internal:1234/v1
LMSTUDIO_TIMEOUT_SECONDS=120

OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_TIMEOUT_SECONDS=120

# OpenAI-compatible API (optional)
OPENAI_COMPAT_BASE_URL=
OPENAI_API_KEY=
OPENAI_TIMEOUT_SECONDS=120

# =============================================================================
# SSE STREAMING
# =============================================================================
SSE_HEARTBEAT_SECONDS=15
SSE_CLIENT_DISCONNECT_GRACE_SECONDS=2
